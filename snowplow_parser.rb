# Function: Parse Snowplow cloudfront logs and insert them into a Redshift database
# Author: Bobby Biswal

require 'rubygems'
require 'bundler/setup'

require 'aws-sdk-s3'
require 'zlib'
require 'open-uri'
require 'cgi'
require 'rack'
require 'redshift/client'
require 'pry'
require 'timeout'

# Set these variables for the specific S3 Bucket config
BUCKET_REGION = 'us-west-2'
BUCKET_CLOUDFRONT_LOGS = 'cloudfront-logs'
BUCKET_PARSED_LOGS = 'cloudfront-logs-parsed'
BUCKET_OBJECT_LIMIT = 150  	# The number of cloudfront log objects that should retrieved at a time
WAIT_TIME = 60  			# Time to wait for s3 parsed log upload and redshift copy

# Fields common to all event types
FIELDS = [	
			"evid","ts","aid","ds","dtm","duid","eid","fp",
			"p","portal_id","sid","uid","res",
			"vp","tz","vid","user_agent",
			"refr","lang","url"
		 ]

# Specify the column ordering of the various event types to match the redshift column ordering
# Change this to match your specific event types.  
LOGIN_FIELDS = ["evid","ts","aid","dtm","eid","fp","p","portal_id","sid",
				"login_method","tz","ds","lang","duid","vp","uid","vid","url","user_agent","res","refr"]

ADD_TO_CART_FIELDS = ["evid","ts","aid","dtm","duid","eid","fp","p","portal_id","sid","uid","res","ds",
					  "vp","lang","tz","url","sku","user_agent","vid","refr"]

CART_CONTENTS_FIELDS = ["evid","ts","aid","ds","dtm","duid","eid","fp","p","portal_id","res","sid","uid",
						"vp","sku","lang","tz","url","user_agent","vid","refr"]

DELETE_FROM_CART_FIELDS = ["evid","ts","aid","dtm","duid","eid","fp","p","portal_id","res","sid","uid","vp",
	"ds","vid","lang","url","refr","user_agent","tz","sku"]						

PAGE_VIEW_FIELDS = ["evid","ts","aid","dtm","duid","eid","fp","p","portal_id","sid","tz","uid",
					"vp","ds","_sm_au_","refr","user_agent","vid","url","lang","res","page"]

WATCH_VIDEO_FIELDS = ["evid","ts","aid","ds","dtm","duid","eid","fp","p","portal_id","sid","uid","res",
					  "video_action","vp","video_name","tz","vid","user_agent","refr","lang","url"]

FILE_MAX_LINES = 500000	# Maximum lines to insert into a parsed log file

# Determine the event type.  For a snowplow event guide, refer to:
# https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol
def get_event_type r
	type = nil
	if r["e"]=="se" and r["se_ca"]
		j = JSON.parse( CGI.unescape(r["se_ca"]))
		if j["evname"]
			type = j["evname"]
		end
	elsif r["e"] == "pv"
		type = "page_view"		
	end
	type = nil if type == "undefined"
	return type
end

# Quotes a string, escaping any ' (single quote) and \ (backslash) characters.
def escape_string(s)
  	return "" if s.nil?
  	s = CGI.unescape(s)
	s.gsub(/\\/, '\&\&').gsub(/'/, "''") # ' (for ruby-mode)
end

def get_se_param r, param
	if r and r["se_ca"]
		str = JSON.parse( CGI.unescape(r["se_ca"]))[param]
		# remove any leading or trailing quotes
		str = str.gsub(/\A\'/,"").gsub(/\'\z/,"")
		#puts str
		str
	else
		""
	end
end

# Create a line in the order defined by the fields array
def get_line_from_fields(r, fields)
	return "" if fields.nil?
	str = ""

	fields.each{|field|
		str += "#{r[field]}|"
	}
	str.chomp("|")
end

# Create a parsed log line for a specific event type
def create_copy_file_line(r)

	event_type = get_event_type(r)
	if event_type.nil?
		# Sometimes cloudfront may get access logs that are not generated by Snowplow
		#puts "WARNING: Couldn't get event type for #{r['evid']}"
		return
	end

	table_name = "_e_portal__#{event_type}"
	fields = nil

	if event_type == "login"
		r["login_method"] = "#{get_se_param(r,'login_method')}"
		@login_lines.push(get_line_from_fields(r,  LOGIN_FIELDS))
	elsif event_type == "watch_video"
		r["video_action"] = "#{get_se_param(r,'video_action')}"
		r["video_name"] = "#{get_se_param(r,'video_name')}"		
		@watch_video_lines.push(get_line_from_fields(r,  WATCH_VIDEO_FIELDS))
	elsif event_type == "add_to_cart"
		r["sku"] = "#{get_se_param(r,'sku')}"
		@add_to_cart_lines.push(get_line_from_fields(r,  ADD_TO_CART_FIELDS))
	elsif event_type == "delete_from_cart"
		r["sku"] = "#{get_se_param(r,'sku')}"
		@delete_from_cart_lines.push(get_line_from_fields(r,  DELETE_FROM_CART_FIELDS))
	elsif event_type == "cart_contents"
		r["sku"] = "#{get_se_param(r,'sku')}"
		@cart_contents_lines.push(get_line_from_fields(r,  CART_CONTENTS_FIELDS))
	elsif event_type == "page_view"
		@page_view_lines.push(get_line_from_fields(r,  PAGE_VIEW_FIELDS))
	end

end

# build_field_idx creates a hash table of field names to indicies in a log line
def get_field_idx line
	field_idx = {}
	count = 0
	field_names = line.split("\s")
	field_names.each{|field_name|
		field_idx[field_name] = count
		count += 1
	}

	field_idx		
end

# parse_snowplow_log_line extracts certain fields from the snowplow query
# and the cloudfront/S3 access log.  The query fields are adjusted
# based on the specific Redshift event schema
def parse_snowplow_log_line(line, field_idx)

	return nil if line.nil? or line == ""

	r = nil

	begin
		fields = line.split("\s")

		# create hash of snowplow and other url query params
		r = Rack::Utils.parse_query(fields[field_idx["cs-uri-query"]])

		# Build variables from cloudfront-specific fields
		r["ts"] = "#{fields[field_idx["date"]]} #{fields[field_idx["time"]]}"
		r["evid"] = r["ts"].gsub(/[-\s:]/,"") + "000Z" + ('A'..'Z').to_a.shuffle[0,22].join
		r["refr"] = fields[field_idx["cs(Referer)"]]
		r["ip_address"] = fields[field_idx["c-ip"]]
		r["user_agent"] = fields[field_idx["cs(User-Agent)"]]

		# escape all the fields
		r.update(r){|k,v| escape_string(v)}
	rescue Exception => e
		puts "There was a problem parsing a log line: #{e.message}\n#{e.backtrace}"
		r = nil
	end

	r

end

def gzip(data)
  str = StringIO.new
  gz = Zlib::GzipWriter.new(str)
  gz.write(data)
  gz.close
  str.string
end

# Generate the json manifest file for the redshift COPY operation
def get_manifest_json(s3_filenames)
	json = "{\"entries\": [\n"
   	s3_filenames.each{|s3_filename|
		json +=  "  {\"url\":\"s3://#{BUCKET_PARSED_LOGS}/#{s3_filename}\"}\n"
	}
	json += "]}"
	json
end

def create_and_upload_log_file(lines, type)
	return false if lines.empty? or lines.nil?
	begin
		s3_filenames = []
		part_suffix = 0
		while !lines.empty?
			lines_data = ""
			gz_lines = lines.shift(FILE_MAX_LINES)
			s3_filename = "#{type}/#{type}_#{@bucket_create_suffix}_#{part_suffix}.gz"

			s3_filenames.push(s3_filename)
			s3_data = gzip(gz_lines.join("\n").chomp("\n"))

			# Timeout s3 to prevent hanging the program
			status = Timeout::timeout(WAIT_TIME) {
				obj = @s3.bucket(BUCKET_PARSED_LOGS).object(s3_filename)
				obj.put(body: s3_data)
			}

			puts "Wrote parsed log: #{s3_filename}"
			part_suffix += 1
		end

		# Upload the manifest file
		manifest_json = get_manifest_json(s3_filenames)

		# Timeout s3 to prevent hanging the program
		status = Timeout::timeout(WAIT_TIME) {
			obj = @s3.bucket(BUCKET_PARSED_LOGS).object("#{type}/#{type}.manifest")
			obj.put(body: manifest_json)

			# Copy to redshift
			copy_s3_to_redshift(type)

			# Clean-up parsed log bucket
			objects = @s3.bucket(BUCKET_PARSED_LOGS).objects({prefix: type})
			objects.batch_delete!
		}
	rescue Exception => e
		puts "Error during create_and_upload_log_file: #{e.message} #{e.backtrace}"
		return false
	end


	return true
end

# Copy the parsed logs to the Redshift database
def copy_s3_to_redshift(type)
	query = "COPY _e_portal__#{type}
	  FROM 's3://#{BUCKET_PARSED_LOGS}/#{type}/#{type}.manifest'
	  WITH CREDENTIALS 
	        'aws_access_key_id=#{ENV['AWS_ACCESS_KEY_ID']};aws_secret_access_key=#{ENV['AWS_SECRET_ACCESS_KEY']}'
	  REGION 'us-west-2'
	  MANIFEST
	  GZIP
	  ACCEPTANYDATE
	  TRUNCATECOLUMNS
	  ACCEPTINVCHARS"

	result = Redshift::Client.connection.exec(query)
	if result.result_status != 1
		puts "ERROR: An error occurred while loading the redshift data: #{result.error_message}"
		false
	else
		true
	end
end

# Remove the cloudfront logs that have been parsed
def remove_s3_cloudfront_log_files log_filenames
	return if log_filenames.empty?
	puts "Removing cloudfront logs from s3 #{BUCKET_CLOUDFRONT_LOGS} that have been parsed"

	log_filenames.each{|log_filename|
		@s3.bucket(BUCKET_CLOUDFRONT_LOGS).object(log_filename).delete
	}
end

def add_parsed_logs_to_s3
	# create the log files
	result = create_and_upload_log_file(@login_lines, "login")
	return false if !result
	create_and_upload_log_file(@watch_video_lines, "watch_video")
	create_and_upload_log_file(@add_to_cart_lines, "add_to_cart")
	create_and_upload_log_file(@cart_contents_lines, "cart_contents")
	create_and_upload_log_file(@delete_from_cart_lines, "delete_from_cart")
	create_and_upload_log_file(@page_view_lines, "page_view")
	return true
end


# Note: REDSHIFT_URL, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY should be set as environment variables
@s3 = Aws::S3::Resource.new(region: BUCKET_REGION)

bucket = @s3.bucket(BUCKET_CLOUDFRONT_LOGS)

Redshift::Client.establish_connection

Redshift::Client.connection

@login_lines = []
@add_to_cart_lines = []
@delete_from_cart_lines = []
@cart_contents_lines = []
@watch_video_lines = []
@page_view_lines = []
@bucket_create_suffix = Time.now.strftime("%Y-%m-%d.%H%M%S")

insert_count = 0
log_filenames = []

puts "Starting snowplow parser (#{Time.now})"

bucket.objects.limit(BUCKET_OBJECT_LIMIT).each do |item|
  puts "Parsing cloudfront log file:  #{item.key}"
  log_filenames.push(item.key)
  field_idx = nil
  url = item.presigned_url(:get)
  begin
	  open(url) do |log_gz|
	  	log = Zlib::GzipReader.new(log_gz)
	  	headers = nil
	  	log.each_line do |line|
	  		# Skip the headers in the file
	  		next if line.match(/\A#(Version)/)

	  		# Build the headers name to index mapping
	  		if  line.match(/\A#Fields: (.*)/) and field_idx.nil?
	  			field_idx = get_field_idx($1)
	  		# Otherwise parse the line and insert the fields into the Redshift DB
	  		else
	  			r = parse_snowplow_log_line(line, field_idx)
	  			unless r.nil?
	  				create_copy_file_line(r)
	  			end
	  		end

	  	end
	  end
   rescue Exception=>e
   	 puts "Error on #{item.key}: #{e.message} #{e.backtrace}"
   	 log_filenames -= [item.key]  # don't delete the log file if there was an open error
   end
end

# create the parsed log files and copy it to the redshift table
unless log_filenames.empty?
	if add_parsed_logs_to_s3
		remove_s3_cloudfront_log_files(log_filenames)
	end
else
	puts "No cloudfront log files found in S3 bucket #{BUCKET_CLOUDFRONT_LOGS}"
end